---
layout: post
title: "Lec2 Word Vector"
category: NLP, CS224n
tags: []
date:   2019-06-10 13:25:35 +0200
---

之前使用的表示 word 的方法是 one-hot representation

但是这种方法一个缺陷是维度过高，另外无法表示词语之间的相似性(因为词与词之间的向量正交 orthogonal)

所以想要找到一种方法能够直接 encode 词语之间的 similarity

> You shall know a word by the company it keeps. ——J.R. Firth

我们能从一个词周围的信息（neighbor/context）来表示这个词

$$p(context\|w_t)$$  

Loss function: $$J=1-p(w_{-t}\|w_t)$$

通过不断的调整词向量的值来减小 loss

Basic Idea of Word2Vec:

> Predict between every word and its context words

两个算法：

1. Skip-grams (SG):  给定 target word, 预测 context words
2. Continuous Bag of Words (CBOW): 从 bag-of-words context 预测 target words



两种训练方法：

1. Hierarchical Softmax
2. Negative Sampling
3. Naive Softmax



## Skip-gram

给定 target/center word $$w_t$$，预测以 m 为窗口大小的左侧 context words $$P(w_{t-m}\|w_t)... P(w_{t-1}\|w_t)$$ 和右侧 context words $$P(w_{t+1}\|w_t)... P(w_{t+m}\|w_t)$$ 

Loss function/cost function/objective function：给定 current center word, 最大化 context word 的概率

$$J'(\theta)=\prod_{t=1}^T \prod_{-m \leq j \leq m, j \neq 0} P(w_{t+j}\|w_t;\theta)$$ 

Negative Log Likelihood: $$J(\theta)=-\frac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j}\|w_t)$$

$$P(w_{t+j}\|w_t)$$最简单的形式是：$$P(o\|c)=softmax(u_o^T v_c)$$

其中：o 是 outside word index，c 是 center word index，$$v_c$$和$$u_o$$是"center"和"outside" vectors

每个词有两个向量，一个是作为 center word 时的向量，一个是作为 outside word 时的向量，两个向量互相独立

Dot product: $$u^Tv$$，如果两个向量 u 和 v 越相似，那么点乘的值越大，所以$$P(o\|c)$$是在计算两个词之间的相似度

softmax：standard map from $$\mathbf{R}^V$$ to a probability distribution

![](https://strongman1995.github.io/assets/images/2019-06-10-word2vec/1.png)

为了训练模型，需要计算所有的 vector gradients，$$\theta = [v_1, …, v_{\|V\|}, u_1, …, u_{\|V\|}]\in \mathbb{R}^{2dV}$$ 

每个单词都有两个 vectors。

