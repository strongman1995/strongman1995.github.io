---
layout: post
title: "Transformer"
category: DL
tags: []
date: 2019-08-12 13:25:35 +0200
---

# Transformer

从**编码器**输入的句子首先会经过一个**自注意力（self-attention）层**，这层帮助编码器在对每个单词编码时关注输入句子的其他单词。自注意力层的输出会传递到**前馈（feed-forward）神经网络**中。每个位置的单词对应的前馈神经网络都完全一样（译注：另一种解读就是一层窗口为一个单词的一维卷积神经网络）。

**解码器**中也有编码器的**自注意力（self-attention）层**和**前馈（feed-forward）层**。除此之外，这两个层之间还有一个**注意力层**，用来关注输入句子的相关部分（和seq2seq模型的注意力作用相似）。

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/1.jpg)

word2vec过程只发生在**最底层的编码器**中。所有的编码器都有一个相同的特点，即它们接收一个向量列表，列表中的每个向量大小为512维。在底层（最开始）编码器中它就是词向量，但是在其他编码器中，它就是下一层编码器的输出（也是一个向量列表）。向量列表大小是我们可以设置的超参数——一般是我们训练集中最长句子的长度。

Transformer的一个核心特性：在这里输入序列中每个位置的单词都有自己独特的路径流入编码器。在自注意力层中，这些路径之间存在依赖关系。而前馈（feed-forward）层没有这些依赖关系。因此在前馈（feed-forward）层时可以并行执行各种路径。

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/2.jpg)





![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/3.jpg)

这样自注意力的计算就完成了。得到的向量就可以传给前馈神经网络。然而实际中，这些计算是以矩阵形式完成的，以便算得更快。那我们接下来就看看如何用矩阵实现的。

### **通过矩阵运算实现自注意力机制**

第一步是计算查询矩阵、键矩阵和值矩阵。为此，我们将将输入句子的词嵌入装进矩阵X中，将其乘以我们训练的权重矩阵$$(W^Q，W^K，W^V)$$。

X 矩阵中的每一行对应于输入句子中的一个单词。我们再次看到词嵌入向量 (512，或图中的4个格子) 和 q/k/v 向量 (64，或图中的3个格子) 的大小差异。

最后，由于我们处理的是矩阵，我们可以将步骤2到步骤6合并为一个公式来计算自注意力层的输出。

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/4.jpg)

![5](https://strongman1995.github.io/assets/images/2019-08-12-transformer/5.jpg)



通过增加一种叫做 **“多头”注意力（“multi-headed” attention）**的机制，论文进一步完善了自注意力层，并在两方面提高了注意力层的性能：

1. 它扩展了模型专注于**不同位置**的能力。在上面的例子中，虽然每个编码都在z1中有或多或少的体现，但是它可能被实际的单词本身所支配。如果我们翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意机制会起到作用。

2. 它给出了**注意力层的多个“表示子空间”（representation subspaces）**。接下来我们将看到，对于“多头”注意机制，我们有多个查询/键/值权重矩阵集 (Transformer使用八个注意力头，因此我们对于每个编码器/解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器/解码器的向量)投影到不同的表示子空间中。

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/6.jpg)

在“多头”注意机制下，我们为每个头保持独立的查询/键/值权重矩阵，从而产生不同的查询/键/值矩阵。和之前一样，我们拿X乘以$$W^Q/W^K/W^V$$矩阵来产生查询/键/值矩阵。

如果我们做与上述相同的自注意力计算，只需八次不同的权重矩阵运算，我们就会得到八个不同的Z矩阵。

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/7.jpg)

前馈层不需要8个矩阵，它只需要一个矩阵(由每一个单词的表示向量组成)。所以我们需要一种方法把这八个矩阵压缩成一个矩阵。那该怎么做？其实可以直接把这些矩阵拼接在一起，然后用一个附加的权重矩阵$$W^O$$与它们相乘。

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/8.jpg)

这几乎就是多头自注意力的全部。这确实有好多矩阵，我们试着把它们集中在一个图片中，这样可以一眼看清。

既然我们已经摸到了注意力机制的这么多“头”，那么让我们重温之前的例子，看看我们在例句中编码“it”一词时，不同的注意力“头”集中在哪里：

当我们编码“it”一词时，一个注意力头集中在“animal”上，而另一个则集中在“tired”上，从某种意义上说，模型对“it”一词的表达在某种程度上是“animal”和“tired”的代表。

然而，如果我们把所有的attention都加到图示里，事情就更难解释了：

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/9.jpg)

### **使用位置编码表示序列的顺序**

到目前为止，我们对模型的描述缺少了一种理解输入单词顺序的方法。为了解决这个问题，Transformer 为每个输入的词嵌入添加了一个向量。这些向量遵循模型学习到的特定模式，这有助于确定每个单词的位置，或序列中不同单词之间的距离。这里的直觉是，将**位置向量**添加到**词嵌入**中使得它们在接下来的运算中，能够更好地表达的词与词之间的距离。

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/10.jpg)

为了让模型理解单词的顺序，我们添加了**位置编码向量**，这些向量的值遵循特定的模式。如果我们假设词嵌入的维数为4，则实际的位置编码如下：

每一行对应一个词向量的位置编码，所以第一行对应着输入序列的第一个词。每行包含512个值，每个值介于1和-1之间。我们已经对它们进行了颜色编码，所以图案是可见的。

20字(行)的位置编码实例，词嵌入大小为512(列)。你可以看到它从中间分裂成两半。这是因为左半部分的值由一个函数(使用正弦)生成，而右半部分由另一个函数(使用余弦)生成。然后将它们拼在一起而得到每一个位置编码向量。

原始论文里描述了位置编码的公式(第3.5节)。你可以在 get_timing_signal_1d()中看到生成位置编码的代码。这不是唯一可能的位置编码方法。然而，它的优点是能够扩展到未知的序列长度(例如，当我们训练出的模型需要翻译远比训练集里的句子更长的句子时)。

### **残差模块**

在继续进行下去之前，我们需要提到一个编码器架构中的细节：在每个编码器中的每个子层（自注意力、前馈网络）的周围都有一个残差连接，并且都跟随着一个“层-归一化”步骤。

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/11.jpg)

解码器的子层也是这样样的。如果我们想象一个2 层编码-解码结构的transformer，它看起来会像下面这张图一样：

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/12.jpg)

### 解码组件

既然我们已经谈到了大部分编码器的概念，那么我们基本上也就知道解码器是如何工作的了。但最好还是看看解码器的细节。

编码器通过处理**输入序列**开启工作。顶端编码器的输出之后会变转化为一个包含**向量K（键向量）**和 **V（值向量）**的注意力向量集 。这些向量将被每个解码器用于自身的“**编码-解码注意力层**”，而这些层可以帮助解码器关注输入序列哪些位置合适：

在完成编码阶段后，则开始解码阶段。

解码阶段的每个步骤都会输出一个输出序列（在这个例子里，是英语翻译的句子）的元素

接下来的步骤重复了这个过程，直到到达一个特殊的终止符号，它表示transformer的解码器已经完成了它的输出。

每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果 。

另外，就像我们对编码器的输入所做的那样，我们会嵌入并添加位置编码给那些解码器，来表示每个单词的位置。

而那些解码器中的自注意力层表现的模式与编码器不同：在解码器中，自注意力层只被允许处理输出序列中更靠前的那些位置，在softmax步骤前，它会把后面的位置给隐去（把它们设为-inf）。

这个“编码-解码注意力层”工作方式基本就像多头自注意力层一样，只不过它是通过在它下面的层来创造查询矩阵，并且从编码器的输出中取得键/值矩阵。

### **最终的线性变换和Softmax层**

解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是Softmax层。

线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里。

不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量——每个单元格对应某一个单词的分数。

接下来的Softmax 层便会把那些分数变成概率（都为正数、上限1.0）。概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。

这张图片从底部以解码器组件产生的输出向量开始。之后它会转化出一个输出单词。

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/13.jpg)

### **损失函数**

比如说我们正在训练模型，现在是第一步，一个简单的例子——把“merci”翻译为“thanks”。这意味着我们想要一个表示单词“thanks”概率分布的输出。但是因为这个模型还没被训练好，所以不太可能现在就出现这个结果。

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/14.jpg)

因为模型的参数（权重）都被随机的生成，（未经训练的）模型产生的概率分布在每个单元格/单词里都赋予了随机的数值。我们可以用真实的输出来比较它，然后用反向传播算法来略微调整所有模型的权重，生成更接近结果的输出。

你会如何比较两个概率分布呢？我们可以简单地用其中一个减去另一个。更多细节请参考**交叉熵**和**KL散度**。

但注意到这是一个过于简化的例子。更现实的情况是处理一个句子。例如，输入“je suis étudiant”并期望输出是“i am a student”。那我们就希望我们的模型能够成功地在这些情况下输出概率分布：

每个概率分布被一个以词表大小（我们的例子里是6，但现实情况通常是3000或10000）为宽度的向量所代表。第一个概率分布在与“i”关联的单元格有最高的概率，第二个概率分布在与“am”关联的单元格有最高的概率以此类推

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/15.jpg)

依据例子训练模型得到的目标概率分布。在一个足够大的数据集上充分训练后，我们希望模型输出的概率分布看起来像这个样子：

![](https://strongman1995.github.io/assets/images/2019-08-12-transformer/16.jpg)

我们期望训练过后，模型会输出正确的翻译。当然如果这段话完全来自训练集，它并不是一个很好的评估指标。注意到每个位置（词）都得到了一点概率，即使它不太可能成为那个时间步的输出——这是softmax的一个很有用的性质，它可以帮助模型训练。

- 因为这个模型一次只产生一个输出，不妨假设这个模型只选择概率最高的单词，并把剩下的词抛弃。这是其中一种方法（叫**贪心解码**）。

- 另一个完成这个任务的方法是留住概率最靠高的两个单词（例如I和a），那么在下一步里，跑模型两次：其中一次假设第一个位置输出是单词“I”，而另一次假设第一个位置输出是单词“me”，并且无论哪个版本产生更少的误差，都保留概率最高的两个翻译结果。然后我们为第二和第三个位置重复这一步骤。这个方法被称作**集束搜索（beam search）**。在我们的例子中，集束宽度是2（因为保留了2个集束的结果，如第一和第二个位置），并且最终也返回两个集束的结果（top_beams也是2）。这些都是可以提前设定的参数。

# Reference

[1]: https://baijiahao.baidu.com/s?id=1622064575970777188&amp;wfr=spider&amp;for=pc
[2]: https://jalammar.github.io/illustrated-transformer/

