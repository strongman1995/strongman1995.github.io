<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Starry, starry night</title>
    <description>Learn from yesterday, live for today, hope for tomorrow</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 18 May 2019 13:53:27 +0800</pubDate>
    <lastBuildDate>Sat, 18 May 2019 13:53:27 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>leetcode 学习笔记</title>
        <description>&lt;h2 id=&quot;2-两数相加&quot;&gt;&lt;a href=&quot;https://leetcode-cn.com/problems/add-two-numbers/&quot;&gt;2. 两数相加&lt;/a&gt;&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Solution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;addTwoNumbers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ListNode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ListNode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ListNode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ListNode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;carry&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;carry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;divmod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;carry&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ListNode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;carry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ListNode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-无重复字符的最长子串&quot;&gt;&lt;a href=&quot;https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/&quot;&gt;3. 无重复字符的最长子串&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;暴力法：O(n^3)
滑动窗口: O(n)， 通过使用 HashSet 作为滑动窗口，我们可以用O(1)的时间来完成对字符是否在当前的子字符串中的检查。滑动窗口是数组/字符串问题中常用的抽象概念。 窗口通常是在数组/字符串中由开始和结束索引定义的一系列元素的集合，即 [i,j)（左闭，右开）。而滑动窗口是可以将两个边界向某一方向“滑动”的窗口。例如，我们将 [i,j)向右滑动 11 个元素，则它将变为 [i+1,j+1)（左闭，右开）。
回到我们的问题，我们使用 HashSet 将字符存储在当前窗口 [i,j)最初 （j=i）中。 然后我们向右侧滑动索引 j，如果它不在 HashSet 中，我们会继续滑动 j。直到 s[j] 已经存在于 HashSet 中。此时，我们找到的没有重复字符的最长子字符串将会以索引 i 开头。如果我们对所有的 i 这样做，就可以得到答案。&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Solution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lengthOfLongestSubstring&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# i 是开始位置&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 原来开始的位置和s[j]重复的位置，取大的那个&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 更新当前字符在 hashset 中存储的位置，hashset 存储了一个窗口中字符的位置&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;5-最长回文子串&quot;&gt;&lt;a href=&quot;https://leetcode-cn.com/problems/longest-palindromic-substring/&quot;&gt;5. 最长回文子串&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;可以用动态规划
但更优的是中心扩展算法，回文可以从它的中心展开，并且只有 2n−1 个这样的中心。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Solution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;longestPalindrome&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;len1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_around_center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;len2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_around_center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;m_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;expand_around_center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Wed, 12 Dec 2018 19:25:35 +0800</pubDate>
        <link>http://localhost:4000/leetcode/2018/12/12/leetcode.html</link>
        <guid isPermaLink="true">http://localhost:4000/leetcode/2018/12/12/leetcode.html</guid>
        
        
        <category>leetcode</category>
        
      </item>
    
      <item>
        <title>概率图模型——inference as optimization: structured variational inference 结构变分推断</title>
        <description>&lt;h2 id=&quot;variational-inferences&quot;&gt;Variational inferences&lt;/h2&gt;
&lt;h2 id=&quot;mean-field-variational-inferences&quot;&gt;Mean field variational inferences&lt;/h2&gt;
&lt;h2 id=&quot;structured-variational-inferences&quot;&gt;Structured variational inferences&lt;/h2&gt;
&lt;h2 id=&quot;deep-variational-autocoders&quot;&gt;Deep variational autocoders&lt;/h2&gt;
</description>
        <pubDate>Fri, 09 Nov 2018 19:25:35 +0800</pubDate>
        <link>http://localhost:4000/pgm/2018/11/09/variational-inference.html</link>
        <guid isPermaLink="true">http://localhost:4000/pgm/2018/11/09/variational-inference.html</guid>
        
        
        <category>pgm</category>
        
      </item>
    
      <item>
        <title>概率图学习——Inference as Optimization: Cluster Graph &amp; Belief Propagation 聚类图，置信传播</title>
        <description>&lt;h2 id=&quot;variable-eliminationve-消元法&quot;&gt;Variable Elimination(VE) 消元法&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-26-inference-as-optimization/1.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;线性链上的消元：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-26-inference-as-optimization/2.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;按照 A、B、C、D 的顺序依次消去&lt;/p&gt;

&lt;h3 id=&quot;ve-in-complex-graphs&quot;&gt;VE in complex graphs&lt;/h3&gt;

&lt;p&gt;induced graph in VE&lt;/p&gt;

&lt;p&gt;step 1: Moralizing for BN （即在 v-structure 的两个父亲节点连边）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-26-inference-as-optimization/3.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-26-inference-as-optimization/4.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;step 2: Triangulation （即在消元过程中做三角化操作）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-26-inference-as-optimization/5.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-26-inference-as-optimization/6.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最后出来会是 chordal，即是一个弦图，图中只有三角，没有四边形&lt;/p&gt;

&lt;p&gt;如果每次 inference 的时候都要遍历整个图，那就太蠢了，这里可以采用动态规划的算法，消元时候的中间结果是可以拿来重用的。&lt;/p&gt;

&lt;p&gt;提前计算好定义在每个 clique 上的 marginal distribution，在做 inference 时候也能快很多。&lt;/p&gt;

&lt;h2 id=&quot;exact-inference-clique-tree&quot;&gt;Exact Inference: Clique Tree&lt;/h2&gt;

&lt;h3 id=&quot;cluster-graph-and-clique-tree&quot;&gt;Cluster graph and clique tree&lt;/h3&gt;

&lt;p&gt;clique tree 是定义在弦图上的，根据 clique 生成的树状结构。&lt;/p&gt;

&lt;p&gt;clique tree 有两个非常重要的性质：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;tree and family preserving: 原来的 induced graph 转化为 clique tree 后具有树状结构，而且和原来的结构是可以相互转换的。clique tree 的每一个节点是代表一个 clique，边上是两个 clique 间重叠的部分。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;running intersection property：是指变量 X 存在一条连续的树的子路径上。如 G 出现在了 Clique2和 clique4中，那么中间的 clique3和 clique5&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-26-inference-as-optimization/7.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-26-inference-as-optimization/8.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每个 clique 都是有他们对应的 local CPD&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-26-inference-as-optimization/9.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;message-passing-sum-product&quot;&gt;Message passing: Sum Product&lt;/h3&gt;

&lt;p&gt;顺序1：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-26-inference-as-optimization/10.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;顺序2：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-26-inference-as-optimization/11.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;clique-tree-calibration&quot;&gt;Clique Tree Calibration&lt;/h3&gt;

&lt;p&gt;calibration（校准）：使得两个相邻 clique 之间传送的消息相等。&lt;/p&gt;

&lt;h3 id=&quot;message-passing-belief-update&quot;&gt;Message passing: Belief Update&lt;/h3&gt;
&lt;h3 id=&quot;constructing-clique-tree3&quot;&gt;Constructing clique tree3&lt;/h3&gt;
</description>
        <pubDate>Fri, 26 Oct 2018 19:25:35 +0800</pubDate>
        <link>http://localhost:4000/pgm/2018/10/26/inference-as-optimization.html</link>
        <guid isPermaLink="true">http://localhost:4000/pgm/2018/10/26/inference-as-optimization.html</guid>
        
        <category>推断</category>
        
        <category>聚类图</category>
        
        <category>置信传播</category>
        
        
        <category>pgm</category>
        
      </item>
    
      <item>
        <title>概率图学习——Learning with incomplete data 从部分观测数据学习</title>
        <description>&lt;h2 id=&quot;variables-are-not-detectable&quot;&gt;Variables are Not Detectable&lt;/h2&gt;

&lt;p&gt;为什么变量是不能观测到的呢？因为变量可能是 hidden variables，不能被观测，只是一个概念，非真实存在。&lt;/p&gt;

&lt;h3 id=&quot;hmm&quot;&gt;HMM&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-18-learning-with-incomplete-data/1.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;状态变量 y 不能被观测&lt;/p&gt;

&lt;h3 id=&quot;gaussian-mixture-model&quot;&gt;Gaussian Mixture Model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-18-learning-with-incomplete-data/2.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;missing-values-and-data-outliers&quot;&gt;Missing Values and Data Outliers&lt;/h2&gt;

&lt;p&gt;缺失值和异常点，系统可能会没有检测到一些观测点。还有一些异常点。&lt;/p&gt;

&lt;h2 id=&quot;learning-in-gaussian-mixture-models&quot;&gt;Learning in Gaussian Mixture Models&lt;/h2&gt;

&lt;p&gt;对一个MLE 框架下，已知数据 D={y[1],…,y[M]}&lt;/p&gt;

&lt;p&gt;目标函数：$\underset{\theta}{\operatorname{arg max}} p(D|\theta)$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-18-learning-with-incomplete-data/3.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于完整数据，MLE 学习是简单的，已知完整数据 Dc={(x[i], y[i])}~i=1…M~&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-18-learning-with-incomplete-data/4.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;E-step:&lt;/p&gt;

&lt;p&gt;x的先验分布π~k~ = p(X=k):  $\pi_k^*=\frac{M[x=k]}{M}$&lt;/p&gt;

&lt;p&gt;M-step:&lt;/p&gt;

&lt;p&gt;$\mu_k^*=\frac{1}{M[x=k]}\sum_m y[m] |_{x[m]=k}$&lt;/p&gt;

&lt;p&gt;$\Sigma_k^&lt;em&gt;=\frac{1}{M[x=k]}\sum_m (y[m]-\mu_k^&lt;/em&gt;)(y[m]-\mu_k^*)^T |_{x[m]=k}$&lt;/p&gt;

&lt;p&gt;但是实际上X 是不知道的，只有 Y 被观测到。&lt;/p&gt;

&lt;p&gt;如果我们知道参数θ，可以求得 X 的后验分布（这是 inference 过程）&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(x=k)=P(x=k\|y, \theta)=\frac{p(y\|x=k, \theta)p(x=k\|\theta)}{\sum_{k=1}^{K} p(y\|x=k, \theta)p(x=k\|\theta)}&lt;/script&gt;

&lt;p&gt;将 x 的先验分布P(X)=π和 Y 的 likelihood P(Y|X=k)=N~k~^(t)^(Y)代入上式, 得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(x[m]=k)=\frac{\pi_k^{(t)}N_k^{(t)}(y[m])}{\sum_{k=1}{K}\pi_k^{(t)}N_k^{(t)}(y[m])}&lt;/script&gt;

&lt;p&gt;其中$N_k^{(y)}=\frac{1}{\sqrt{|2\pi \Sigma|}}exp{-\frac{1}{2}(y-\mu_k)^T\Sigma^{-1}(y-\mu_k)}$&lt;/p&gt;

&lt;p&gt;新一轮的 E 步迭代，使用 MLE 更新&lt;/p&gt;

&lt;p&gt;计算$Q^{(t)}(x[m]=k)$, 代入下面式子&lt;/p&gt;

&lt;p&gt;$\pi_k^{t+1}=\frac{1}{M}\sum_{m=1}^M Q^{(t)}(x[m]=k)$&lt;/p&gt;

&lt;p&gt;新一轮的 M 步迭代：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-18-learning-with-incomplete-data/5.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;general-principles-and-methods&quot;&gt;General Principles and Methods&lt;/h2&gt;
&lt;h3 id=&quot;general-priciples&quot;&gt;General Priciples&lt;/h3&gt;
&lt;h3 id=&quot;expectation-maximizationem&quot;&gt;Expectation Maximization(EM)&lt;/h3&gt;
&lt;h3 id=&quot;mcmc-sampling&quot;&gt;MCMC Sampling&lt;/h3&gt;
</description>
        <pubDate>Thu, 18 Oct 2018 19:25:35 +0800</pubDate>
        <link>http://localhost:4000/pgm/2018/10/18/learning-with-incomplete-data.html</link>
        <guid isPermaLink="true">http://localhost:4000/pgm/2018/10/18/learning-with-incomplete-data.html</guid>
        
        <category>参数学习</category>
        
        <category>参数估计</category>
        
        <category>最大似然估计</category>
        
        <category>MLE</category>
        
        
        <category>pgm</category>
        
      </item>
    
      <item>
        <title>概率图学习——Parameter Learning 参数学习</title>
        <description>&lt;p&gt;概率图分为三大部分：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Representation	P &amp;lt;=&amp;gt;{P,G}&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;parent→child structure(BN) &amp;amp; clique(MN)&lt;/li&gt;
      &lt;li&gt;Gaussian model &amp;amp; exponential families&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Inference		P(Y|E=e, θ)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Particle-based inference&lt;/li&gt;
      &lt;li&gt;Inference as optimization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learning&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

 	- &lt;script type=&quot;math/tex&quot;&gt;\underset{\theta}{\operatorname{max}} P(x[1], x[2], ..., x[M]\|\theta)&lt;/script&gt;
 	- P(θ|x[1], x[2], …, x[M])

&lt;h2 id=&quot;learning-basics&quot;&gt;Learning Basics&lt;/h2&gt;

&lt;p&gt;Learning：从观测数据中构建模型。&lt;/p&gt;

&lt;p&gt;根据不同的准则，定义合适的 loss 或者 likelihood function&lt;/p&gt;

&lt;p&gt;准则包括：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;minimum error,&lt;/li&gt;
  &lt;li&gt;probability (maximum likelihood, maximum a posterior )&lt;/li&gt;
  &lt;li&gt;maximum margin&lt;/li&gt;
  &lt;li&gt;compressive sensing&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在概率框架下的学习：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;parameter learning：{x[m]}~m=1-M~|~G~→P(θ|D)&lt;/li&gt;
  &lt;li&gt;Structure learning：{x[m]}~m=1-M~→P(G, θ|D)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Generative Models: 学习 joint probability P(X=x, Y)&lt;/li&gt;
  &lt;li&gt;Discriminative model: 学习conditional probability P(Y|X=x)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;避免 Overfitting&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;如果模型复杂度比训练数据更大，能得到非常“好”的学习，0 empirical risk，但是在 测试集表现会很差&lt;/li&gt;
  &lt;li&gt;增加泛化，需要去惩罚模型的复杂度，分离训练和测试数据&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在理论层面：&lt;strong&gt;模型的复杂度需要适应数据的复杂度&lt;/strong&gt;，loss function 中的正则项&lt;/p&gt;

&lt;p&gt;在经验层面：cross-validation(LOOCV/N-fold CV)，0.632 boostraping(1-e^-1^)&lt;/p&gt;

&lt;p&gt;什么是0.632 bootstrapping？&lt;/p&gt;

&lt;p&gt;从 M 个样本中有放回地抽取 M 次，那么这些数据大概率会有0.632是不重复的数据，这些数据作为 training 集，剩下没有被抽到的作为 test 集。最后 performance：0.368&lt;em&gt;Performance~Train~ + 0.632&lt;/em&gt;Performance~Test~&lt;/p&gt;

&lt;p&gt;判断是否 overfitting: 在 test 集的 performance 是否严重低于training 集的 performance&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数估计&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果一组数据的分布参数不知道，那么这组数据的联合分布不会等于每个数据的概率乘积。假设知道了分布参数，那所有的 samples 是独立同分布的，即 i.i.d&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-10-10-parameter-learning/1.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;最大似然参数估计-maximum-likelihood-parameter-estimationmle&quot;&gt;最大似然参数估计 Maximum Likelihood Parameter Estimation（MLE）&lt;/h2&gt;

&lt;p&gt;首先，什么是 likelihood？&lt;/p&gt;

&lt;p&gt;likelihood 是给定分布参数的 probability 或者是 confidence&lt;/p&gt;

&lt;p&gt;而 log likelihood 更经常被使用，为了更好的计算&lt;script type=&quot;math/tex&quot;&gt;log P(D\|\theta)=\sum_i log \phi_i(x[i];\theta)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MLE&lt;/strong&gt;：找到是的 likelihood 最大的参数赋值θ，参数估计的一种方法，&lt;/p&gt;

&lt;h2 id=&quot;bayesian-parameter-estimation&quot;&gt;Bayesian Parameter Estimation&lt;/h2&gt;

&lt;h2 id=&quot;map-parameter-estimation&quot;&gt;MAP Parameter Estimation&lt;/h2&gt;
</description>
        <pubDate>Wed, 10 Oct 2018 19:25:35 +0800</pubDate>
        <link>http://localhost:4000/pgm/2018/10/10/parameter-learning.html</link>
        <guid isPermaLink="true">http://localhost:4000/pgm/2018/10/10/parameter-learning.html</guid>
        
        <category>参数学习</category>
        
        <category>参数估计</category>
        
        <category>最大似然估计</category>
        
        <category>MLE</category>
        
        
        <category>pgm</category>
        
      </item>
    
      <item>
        <title>概率图学习——Particle-Based Approximate Inference</title>
        <description>&lt;p&gt;Inference的定义：给定部分观察值E=e，求目标变量Y的概率 &lt;script type=&quot;math/tex&quot;&gt;P(Y\|E=e)&lt;/script&gt; 或者 &lt;script type=&quot;math/tex&quot;&gt;\underset{y}{\operatorname{arg max}} P(Y=y\|E=e)&lt;/script&gt; (最大后验概率MAP)&lt;/p&gt;

&lt;p&gt;Particle-Based Approximate Inference （2018-12-22-PBAI/PBAI）最基本的想法是从目标分布中采样x[1], …, x[m]，然后用采样数据去估计函数&lt;script type=&quot;math/tex&quot;&gt;E_p(f) \approx \frac{1}{M} \sum_{m=1}^{M} f(x[m])&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;2018-12-22-PBAI/PBAI的关键是如何从后验分布&lt;script type=&quot;math/tex&quot;&gt;P(x\|E=e)&lt;/script&gt;中采样&lt;/p&gt;

&lt;h2 id=&quot;前向采样-forward-sampling-fs&quot;&gt;前向采样 Forward Sampling （FS）&lt;/h2&gt;

&lt;p&gt;从分布P(X)中利用Bayesian Network产生随机样本，使用&lt;script type=&quot;math/tex&quot;&gt;P(X=e)\approx \frac{1}{M} \sum_{m=1}^{M} I(x[m]=e)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;估计概率， 其中I(·)是指示函数&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;采样过程&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;确定X~1~, … , X~n~的拓扑排序&lt;/li&gt;
  &lt;li&gt;按照拓扑顺序，对每个X~i~进行采样，采样概率&lt;script type=&quot;math/tex&quot;&gt;P(X_i\|pa_i)&lt;/script&gt;，pa~i~的值都已经赋过值了&lt;/li&gt;
  &lt;li&gt;估计概率&lt;script type=&quot;math/tex&quot;&gt;P(X=e)\approx \frac{1}{M} \sum_{m=1}^{M} I(x[m]=e)&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;从而可以估计任何期望&lt;script type=&quot;math/tex&quot;&gt;E_p(f) \approx \frac{1}{M} \sum_{m=1}^{M} f(x[m])&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;如果只需要估计变量X的子集Y，使用&lt;script type=&quot;math/tex&quot;&gt;P(y)\approx \frac{1}{M} \sum_{m=1}^{M} I\{x[m](Y=y)\}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;采样开销&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;每个变量X~i~的开销：O(log(Val|X~i~))， 其中Val|X~i~是指变量X~i~的取值范围&lt;/li&gt;
  &lt;li&gt;每个sample的开销：&lt;script type=&quot;math/tex&quot;&gt;O(nlog(\underset{i}{\operatorname{max}} Val\|X_i))&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;M个sample的开销：&lt;script type=&quot;math/tex&quot;&gt;O(Mnlog(\underset{i}{\operatorname{max}} Val\|X_i))&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;需要sample的数量&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为了以概率&lt;script type=&quot;math/tex&quot;&gt;1-\delta&lt;/script&gt;, 达到相对误差&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt; \epsilon %]]&gt;&lt;/script&gt;，至少需要采样&lt;script type=&quot;math/tex&quot;&gt;M \geq 3 \frac{ln(2/\delta)}{P(y) \epsilon ^ 2}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;如果P(y) ↓，则M↑，才能更精确地观测。&lt;/p&gt;
&lt;h3 id=&quot;forward-sampling-with-rejection&quot;&gt;Forward Sampling with Rejection&lt;/h3&gt;
&lt;p&gt;因为是要在观测到一部分变量值e得情况下求目标变量Y的概率&lt;script type=&quot;math/tex&quot;&gt;P(Y\|E=e)&lt;/script&gt;。用带拒绝的方式做采样，用前向采样采出的数据，如果E≠e，就把这个样本扔掉。从被接受的样本中去估计。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;缺点/问题&lt;/strong&gt; 如果p(e)=0.001,概率非常小，那么扔掉的样本会非常多，浪费很多样本资源。&lt;/p&gt;

&lt;h2 id=&quot;似然采样-likelihood-weighting-lw&quot;&gt;似然采样 Likelihood Weighting （LW）&lt;/h2&gt;
&lt;p&gt;通过FS with rejection的问题，是否可以让所有的样本都满足E=e。&lt;/p&gt;

&lt;p&gt;那么可以把在sample到观测变量X∈E时，直接设置为X=e。原来我们是从后验P(X|e)做sample，现在我们是直接从先验P(X)得到采样。&lt;/p&gt;

&lt;p&gt;所以想从P’(X, e)中得到sample再归一化。&lt;/p&gt;

&lt;p&gt;总结来说就是从先验分布P(X)得到样本，再用likelihood加权样本。&lt;/p&gt;

&lt;p&gt;P(Y, e) = P(Y|e)P(e), 所以P(Y|e)是P(Y, e)的一部分&lt;/p&gt;

&lt;p&gt;根据BN分解定理，&lt;script type=&quot;math/tex&quot;&gt;P(X)=\prod_i P(X_i\|Pa(X_i))&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;观察值给了定值&lt;script type=&quot;math/tex&quot;&gt;E_j=e_j&lt;/script&gt;, 所以每个采样值应该加上权值&lt;script type=&quot;math/tex&quot;&gt;\prod_{j}P(E_j=e_j\|Pa(E_j))&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;和FS with rejection联系：采样可以看成有&lt;script type=&quot;math/tex&quot;&gt;\prod_{j}P(E_j=e_j\|Pa(E_j))&lt;/script&gt;的概率被接受&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;采样过程&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;确定X~1~, … , X~n~的拓扑排序&lt;/li&gt;
  &lt;li&gt;对于每个变量X~i~(采M个样本)
 如果X~i~∉E，直接从&lt;script type=&quot;math/tex&quot;&gt;P(X_i\|Pa_i)&lt;/script&gt;采样
 如果X~i~∈E，设X~i~=E[x~i~]，&lt;script type=&quot;math/tex&quot;&gt;w_i = w_i · P(E[x_i]\|Pa_i)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;得到w~i~，x[1], …, x[M]&lt;/li&gt;
  &lt;li&gt;估计概率&lt;script type=&quot;math/tex&quot;&gt;P(y\|e) \approx \frac{\sum_{m=1}^{M} w[m]I\{x[m](Y=y)\}}{\sum_{m=1}^{M} w[m]}&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;其中&lt;script type=&quot;math/tex&quot;&gt;w[m]=\prod_{x_i \in E}P(x_i=e_i\|Pa_i)&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;重要性采样-importance-sampling-is&quot;&gt;重要性采样 Importance Sampling （IS）&lt;/h2&gt;

&lt;p&gt;估计一个和P相关的函数Q，从Q中采样。P是目标分布，Q是采样分布。&lt;/p&gt;

&lt;p&gt;要求Q：P(x) &amp;gt; 0 → Q(x) &amp;gt; 0&lt;/p&gt;

&lt;p&gt;Q不会漏掉P的任何一个非零概率的事件。&lt;/p&gt;

&lt;p&gt;在实际中，如果Q和P越想似，采样的效果自然是更好。当Q=P时，得到最低的方差估计&lt;/p&gt;

&lt;p&gt;最简单的Q是把P的BN上的边都去掉了，即每个变量都是完全独立的。&lt;/p&gt;

&lt;h3 id=&quot;unnormalized-importance-sampling&quot;&gt;Unnormalized Importance Sampling&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-1.png&quot; alt=&quot;IS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;左半边是从Q做sampling，右半边是对P做sampling&lt;/p&gt;

&lt;p&gt;所以从Q中Sample的数据可以用来近似P的采样&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-2.png&quot; alt=&quot;IS&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;normalized-importance-sampling&quot;&gt;Normalized Importance Sampling&lt;/h3&gt;

&lt;p&gt;归一化P’，P=P’/α，&lt;script type=&quot;math/tex&quot;&gt;\alpha=\sum_x P'(x)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;已知α：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-3.png&quot; alt=&quot;IS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因此可以推导出归一化的P和Q的采样估计&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-4.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以从Q中Sample的数据可以用来近似P的采样&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-5.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;和刚才未归一化的做对比&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-6.png&quot; alt=&quot;IS&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;importance-sampling-for-bayesian-networks&quot;&gt;Importance Sampling for Bayesian Networks&lt;/h3&gt;

&lt;p&gt;定义mutilated network（残支网络）G~E=e~：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;节点X∈E没有parents&lt;/li&gt;
  &lt;li&gt;在节点X∈E的CPD中只有X=E[X]那一项概率为1，其余为0&lt;/li&gt;
  &lt;li&gt;其余节点不变&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-7.png&quot; alt=&quot;mutilated network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果Q定义为mutilated network, 那么LW和IS是相同的采样公式&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Likelihood Weighting&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y\|e) \approx \frac{\sum_{m=1}^{M} w[m]I\{x[m](Y=y)\}}{\sum_{m=1}^{M} w[m]}&lt;/script&gt;

&lt;p&gt;其中&lt;script type=&quot;math/tex&quot;&gt;w[m]=\prod_{x_i \in E}P(x_i=e_i\|Pa_i)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Importance Sampling&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y\|e) \approx \frac{\sum_{m=1}^{M} P'(x[m])/Q(x[m])I\{x[m](Y=y)\}}{\sum_{m=1}^{M} P'(x[m])/Q(x[m])}&lt;/script&gt;

&lt;p&gt;其中&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-8.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;需要sample的数量&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;取决于P和Q的相似度&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;——LW和IS的不足&lt;/p&gt;

&lt;p&gt;LW在Markov Network（MN）很低效，因为需要将MN转化为BN&lt;/p&gt;

&lt;p&gt;IS在选择一个合适的Q上很难，如果Q和P太不像，收敛会很慢&lt;/p&gt;

&lt;h2 id=&quot;蒙特卡洛方法-markov-chain-monte-carlo-methodmcmc&quot;&gt;蒙特卡洛方法 Markov Chain Monte Carlo Method（MCMC）&lt;/h2&gt;

&lt;p&gt;MCMC的基本想法是设计一个马氏链，其稳态分布是P(X|e)，即我们要求的目标分布。所以从这个马氏链上的采样就会服从我们的目标分布。&lt;/p&gt;

&lt;p&gt;通过马氏链的稳态分布来做inference。&lt;/p&gt;

&lt;h3 id=&quot;markov-chain--stationary-distribution&quot;&gt;Markov Chain &amp;amp; Stationary Distribution&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-9.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-10.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一个Markov Chain是regular需要满足链上所有的状态都是在有限k步内可达。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;定理：一个有限状态的马氏链T有一个唯一的稳态分布 当且仅当 T是regular的。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;gibbs-sampling&quot;&gt;Gibbs Sampling&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-11.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;怎么判断Gibbs-sampling MC是regular的呢？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BN：所有的CPD严格为正&lt;/li&gt;
  &lt;li&gt;MN：所有的clique potential严格为正&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;采样过程&lt;/strong&gt;（一个样本）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;设x[m]=x[m-1]且更换变量更新顺序(增加随机性)&lt;/li&gt;
  &lt;li&gt;对每个变量X~i~∈X-E：
    &lt;ul&gt;
      &lt;li&gt;设u~i~为Xi的Markov Blanket&lt;/li&gt;
      &lt;li&gt;从P(X~i~|u~i~)采样X~i~的新值&lt;/li&gt;
      &lt;li&gt;更新X~i~为采样值&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;得到x[m]&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;gibbs-sampling-for-bns&quot;&gt;Gibbs Sampling for BNs&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;采样过程&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-12.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;gibbs-sampling-for-mns&quot;&gt;Gibbs Sampling for MNs&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;采样过程&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://127.0.0.1:4000/assets/images/2018-12-22-PBAI/PBAI-13.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;metropolis-hastings-algorithm&quot;&gt;Metropolis-Hastings Algorithm&lt;/h3&gt;

&lt;p&gt;Gibbs Sampling 给定一个当前状态 S，转移到下一个状态的转移概率是确定的，整个状态转移概率矩阵其实是确定的。&lt;/p&gt;

&lt;p&gt;现在 MH algorithm 追求的是可以从任意一个转移分布中采到下一个样本。和 Importance sampling 有点像，可以从任意的函数 Q 中去拟合 P。&lt;/p&gt;

&lt;p&gt;所以设计了一个新的因子：acceptance probability 接受概率。是指是否接受一个状态转移 A(x→x’)&lt;/p&gt;

&lt;p&gt;现在的转移变成：&lt;/p&gt;

&lt;p&gt;状态 x 转移到状态 x’：T(x→x’) = T^Q^(x→x’)A(x→x’)&lt;/p&gt;

&lt;p&gt;状态 x 停留在原状态：T(x→x) = T^Q^(x→x) + &lt;script type=&quot;math/tex&quot;&gt;\sum_{x'\neq x}&lt;/script&gt;T^Q^(x→x’)(1-A(x→x’))&lt;/p&gt;

&lt;p&gt;上式第一项是原来的转移就是停留在原状态，第二项是指本来要转到其他状态，但是被拒绝了，只能留在原状态。&lt;/p&gt;

&lt;p&gt;因为 MC 是稳态的，所以互相转移的概率是相等的。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(x)T^Q(x→x')A(x→x')=\pi(x')T^Q(x'→x)A(x'→x)&lt;/script&gt;

&lt;p&gt;从上式推出：重要！！！在结构学习的时候会使用到这个结论&lt;script type=&quot;math/tex&quot;&gt;A(x'→x)=min[1, \frac{\pi(x')T^Q(x'→x)}{\pi(x)T^Q(x→x')}]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;在连续概率分布结论也是成立的。经常用到的是把转移概率分布设为多元高斯分布（可以看成是以当前状态为中心的随机游走）。那么此时的转移概率是对称的（随机游走过程），即T(x’→x)=T(x→x’)。&lt;/p&gt;

&lt;p&gt;此时的 acceptance rate 只与稳态分布或者目标分布有关，即&lt;script type=&quot;math/tex&quot;&gt;A(x'→x)=min[1, \frac{\pi(x')}{\pi(x)}]=min[1, \frac{p(x')}{p(x)}]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;MH algorithm 有个缺陷就是不同状态之间的转移太低效了，可能会产生很多 rejection。&lt;/p&gt;

&lt;h3 id=&quot;hybrid-hamiltonian-monte-carlo-hmc&quot;&gt;(Hybrid) Hamiltonian Monte Carlo (HMC)&lt;/h3&gt;
</description>
        <pubDate>Sat, 22 Sep 2018 19:25:35 +0800</pubDate>
        <link>http://localhost:4000/pgm/2018/09/22/PBAI.html</link>
        <guid isPermaLink="true">http://localhost:4000/pgm/2018/09/22/PBAI.html</guid>
        
        <category>采样</category>
        
        <category>推理</category>
        
        <category>MCMC</category>
        
        
        <category>pgm</category>
        
      </item>
    
  </channel>
</rss>
