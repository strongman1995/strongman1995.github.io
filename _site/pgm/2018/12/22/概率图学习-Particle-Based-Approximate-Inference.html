<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Favicon Icon -->
    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.png">

    <title>概率图学习-Particle-Based Approximate Inference</title>
    <meta name="description"
          content="Inference的定义：给定部分观察值E=e，求目标变量Y的概率  或者  (最大后验概率MAP)">

    <link rel="canonical" href="http://localhost:4000/pgm/2018/12/22/%E6%A6%82%E7%8E%87%E5%9B%BE%E5%AD%A6%E4%B9%A0-Particle-Based-Approximate-Inference.html">
    <link rel="alternate" type="application/rss+xml" title="Welcome to my blog" href="http://localhost:4000/feed.xml">

    <script type="text/javascript" src="/bower_components/jquery/dist/jquery.min.js"></script>

    <!-- Third-Party CSS -->
    <link rel="stylesheet" href="/bower_components/bootstrap/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="/bower_components/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="/bower_components/hover/css/hover-min.css">
    <link rel="stylesheet" href="/bower_components/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- My CSS -->
    <link rel="stylesheet" href="/assets/css/common.css">

    <!-- CSS set in page -->
    

    <!-- CSS set in layout -->
    
    <link rel="stylesheet" href="/assets/css/sidebar-post-nav.css">
    

    <script type="text/javascript" src="/bower_components/bootstrap/dist/js/bootstrap.min.js"></script>

</head>


    <body>

    <header class="site-header">
    <div class="container">
        <a id="site-header-brand" href="/" title="Lu Chen">
            <span class="octicon octicon-mark-github"></span> Lu Chen
        </a>
        <nav class="site-header-nav" role="navigation">
            
            <a href="/"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="Home">
                Home
            </a>
            
            <a href="/about"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="About">
                About
            </a>
            
            <a href="/blog"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="Blog">
                Blog
            </a>
            
            <a href="/open-source"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="Open-Source">
                Open-Source
            </a>
            
            <a href="/bookmark"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="Bookmark">
                Bookmark
            </a>
            
        </nav>
    </div>
</header>


        <div class="content">
            <section class="jumbotron geopattern" data-pattern-id="概率图学习-Particle-Based Approximate Inference">
    <div class="container">
        <div id="jumbotron-meta-info">
            <h1>概率图学习-Particle-Based Approximate Inference</h1>
            <span class="meta-info">
                
                
                <span class="octicon octicon-calendar"></span> 2018/12/22
                
            </span>
        </div>
    </div>
</section>
<script>
    $(document).ready(function(){

        $('.geopattern').each(function(){
            $(this).geopattern($(this).data('pattern-id'));
        });

    });
</script>
<article class="post container" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="row">

        
        <div class="col-md-8 markdown-body">

            <p>Inference的定义：给定部分观察值E=e，求目标变量Y的概率 <script type="math/tex">P(Y\|E=e)</script> 或者 <script type="math/tex">\underset{y}{\operatorname{arg max}} P(Y=y\|E=e)</script> (最大后验概率MAP)</p>

<p>Particle-Based Approximate Inference （PBAI）最基本的想法是从目标分布中采样x[1], …, x[m]，然后用采样数据去估计函数<script type="math/tex">E_p(f) \approx \frac{1}{M} \sum_{m=1}^{M} f(x[m])</script></p>

<p>PBAI的关键是如何从后验分布<script type="math/tex">P(x\|E=e)</script>中采样</p>
<h2 id="前向采样-forward-sampling-fs">前向采样 Forward Sampling （FS）</h2>
<p>从分布P(X)中利用Bayesian Network产生随机样本，使用<script type="math/tex">P(X=e)\approx \frac{1}{M} \sum_{m=1}^{M} I(x[m]=e)</script>
估计概率， 其中I(·)是指示函数
<strong>采样过程</strong>：</p>
<ol>
  <li>确定X~1~, … , X~n~的拓扑排序</li>
  <li>按照拓扑顺序，对每个X~i~进行采样，采样概率<script type="math/tex">P(X_i\|pa_i)</script>，pa~i~的值都已经赋过值了</li>
  <li>估计概率<script type="math/tex">P(X=e)\approx \frac{1}{M} \sum_{m=1}^{M} I(x[m]=e)</script></li>
</ol>

<p>从而可以估计任何期望<script type="math/tex">E_p(f) \approx \frac{1}{M} \sum_{m=1}^{M} f(x[m])</script> 
如果只需要估计变量X的子集Y，使用<script type="math/tex">P(y)\approx \frac{1}{M} \sum_{m=1}^{M} I\{x[m](Y=y)\}</script>
<strong>采样开销</strong></p>

<ul>
  <li>每个变量X~i~的开销：O(log(Val|X~i~))， 其中Val|X~i~是指变量X~i~的取值范围</li>
  <li>每个sample的开销：<script type="math/tex">O(nlog(\underset{i}{\operatorname{max}} Val\|X_i))</script></li>
  <li>M个sample的开销：<script type="math/tex">O(Mnlog(\underset{i}{\operatorname{max}} Val\|X_i))</script></li>
</ul>

<p><strong>需要sample的数量</strong></p>

<p>为了以概率<script type="math/tex">1-\delta</script>, 达到相对误差<script type="math/tex">% <![CDATA[
< \epsilon %]]></script>，至少需要采样<script type="math/tex">M \geq 3 \frac{ln(2/\delta)}{P(y) \epsilon ^ 2}</script>
如果P(y) ↓，则M↑，才能更精确地观测。</p>
<h3 id="forward-sampling-with-rejection">Forward Sampling with Rejection</h3>
<p>因为是要在观测到一部分变量值e得情况下求目标变量Y的概率<script type="math/tex">P(Y\|E=e)</script>。用带拒绝的方式做采样，用前向采样采出的数据，如果E≠e，就把这个样本扔掉。从被接受的样本中去估计。
<strong>缺点/问题</strong> 如果p(e)=0.001,概率非常小，那么扔掉的样本会非常多，浪费很多样本资源。</p>

<h2 id="似然采样-likelihood-weighting-lw">似然采样 Likelihood Weighting （LW）</h2>
<p>通过FS with rejection的问题，是否可以让所有的样本都满足E=e。
那么可以把在sample到观测变量X∈E时，直接设置为X=e。原来我们是从后验P(X|e)做sample，现在我们是直接从先验P(X)得到采样。
所以想从P’(X, e)中得到sample再归一化。
总结来说就是从先验分布P(X)得到样本，再用likelihood加权样本。
P(Y, e) = P(Y|e)P(e), 所以P(Y|e)是P(Y, e)的一部分
根据BN分解定理，<script type="math/tex">P(X)=\prod_i P(X_i\|Pa(X_i))</script>
观察值给了定值<script type="math/tex">E_j=e_j</script>, 所以每个采样值应该加上权值<script type="math/tex">\prod_{j}P(E_j=e_j\|Pa(E_j))</script>
和FS with rejection联系：采样可以看成有<script type="math/tex">\prod_{j}P(E_j=e_j\|Pa(E_j))</script>的概率被接受
<strong>采样过程</strong></p>
<ol>
  <li>确定X~1~, … , X~n~的拓扑排序</li>
  <li>对于每个变量X~i~(采M个样本)
 如果X~i~∉E，直接从<script type="math/tex">P(X_i\|Pa_i)</script>采样
 如果X~i~∈E，设X~i~=E[x~i~]，<script type="math/tex">w_i = w_i · P(E[x_i]\|Pa_i)</script></li>
  <li>得到w~i~，x[1], …, x[M]</li>
  <li>估计概率<script type="math/tex">P(y\|e) \approx \frac{\sum_{m=1}^{M} w[m]I\{x[m](Y=y)\}}{\sum_{m=1}^{M} w[m]}</script>
其中<script type="math/tex">w[m]=\prod_{x_i \in E}P(x_i=e_i\|Pa_i)</script>
    <h2 id="重要性采样-importance-sampling-is">重要性采样 Importance Sampling （IS）</h2>
    <p>估计一个和P相关的函数Q，从Q中采样。P是目标分布，Q是采样分布。
要求Q：P(x) &gt; 0 → Q(x) &gt; 0
Q不会漏掉P的任何一个非零概率的事件。
在实际中，如果Q和P越想似，采样的效果自然是更好。当Q=P时，得到最低的方差估计
最简单的Q是把P的BN上的边都去掉了，即每个变量都是完全独立的。</p>
    <h3 id="unnormalized-importance-sampling">Unnormalized Importance Sampling</h3>
    <p><img src="https://img-blog.csdnimg.cn/2018122218345624.png" alt="IS" />
左半边是从Q做sampling，右半边是对P做sampling
所以从Q中Sample的数据可以用来近似P的采样
<img src="https://img-blog.csdnimg.cn/20181222183558234.png" alt="IS" /></p>
    <h3 id="normalized-importance-sampling">Normalized Importance Sampling</h3>
    <p>归一化P’，P=P’/α，<script type="math/tex">\alpha=\sum_x P'(x)</script>
已知α：
<img src="https://img-blog.csdnimg.cn/2018122218410039.png" alt="IS" />
因此可以推导出归一化的P和Q的采样估计
<img src="https://img-blog.csdnimg.cn/20181222184128386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIwNjA3MTg5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" />
所以从Q中Sample的数据可以用来近似P的采样
<img src="https://img-blog.csdnimg.cn/20181222184622950.png" alt="在这里插入图片描述" />
和刚才未归一化的做对比
<img src="https://img-blog.csdnimg.cn/20181222183558234.png" alt="IS" /></p>
    <h3 id="importance-sampling-for-bayesian-networks">Importance Sampling for Bayesian Networks</h3>
    <p>定义mutilated network（残支网络）G~E=e~：</p>
  </li>
</ol>

<ul>
  <li>节点X∈E没有parents</li>
  <li>在节点X∈E的CPD中只有X=E[X]那一项概率为1，其余为0</li>
  <li>其余节点不变</li>
</ul>

<p><img src="https://img-blog.csdnimg.cn/20181222185125204.png" alt="mutilated network" />
如果Q定义为mutilated network, 那么LW和IS是相同的采样公式
<strong>Likelihood Weighting</strong>:
<script type="math/tex">P(y\|e) \approx \frac{\sum_{m=1}^{M} w[m]I\{x[m](Y=y)\}}{\sum_{m=1}^{M} w[m]}</script>
其中<script type="math/tex">w[m]=\prod_{x_i \in E}P(x_i=e_i\|Pa_i)</script>
<strong>Importance Sampling</strong>
<script type="math/tex">P(y\|e) \approx \frac{\sum_{m=1}^{M} P'(x[m])/Q(x[m])I\{x[m](Y=y)\}}{\sum_{m=1}^{M} P'(x[m])/Q(x[m])}</script>
其中<img src="https://img-blog.csdnimg.cn/20181222190212660.png" alt="在这里插入图片描述" />
<strong>需要sample的数量</strong>
取决于P和Q的相似度
<strong>总结</strong>——LW和IS的不足
LW在Markov Network（MN）很低效，因为需要将MN转化为BN
IS在选择一个合适的Q上很难，如果Q和P太不像，收敛会很慢</p>
<h2 id="蒙特卡洛方法-markov-chain-monte-carlo-methodmcmc">蒙特卡洛方法 Markov Chain Monte Carlo Method（MCMC）</h2>
<p>MCMC的基本想法是设计一个马氏链，其稳态分布是P(X|e)，即我们要求的目标分布。所以从这个马氏链上的采样就会服从我们的目标分布。
通过马氏链的稳态分布来做inference。</p>
<h3 id="markov-chain--stationary-distribution">Markov Chain &amp; Stationary Distribution</h3>
<p><img src="https://img-blog.csdnimg.cn/20181222190802777.png" alt="在这里插入图片描述" />
<img src="https://img-blog.csdnimg.cn/20181222190817152.png" alt="在这里插入图片描述" />
一个Markov Chain是regular需要满足链上所有的状态都是在有限k步内可达。
<strong>定理：一个有限状态的马氏链T有一个唯一的稳态分布 当且仅当 T是regular的。</strong></p>

<h3 id="gibbs-sampling">Gibbs Sampling</h3>
<p><img src="https://img-blog.csdnimg.cn/20181222191750195.png" alt="在这里插入图片描述" />
怎么判断Gibbs-sampling MC是regular的呢？</p>

<ul>
  <li>BN：所有的CPD严格为正</li>
  <li>MN：所有的clique potential严格为正</li>
</ul>

<p><strong>采样过程</strong>（一个样本）</p>
<ol>
  <li>设x[m]=x[m-1]且更换变量更新顺序(增加随机性)</li>
  <li>对每个变量X~i~∈X-E：
    <ul>
      <li>设u~i~为Xi的Markov Blanket</li>
      <li>从P(X~i~|u~i~)采样X~i~的新值</li>
      <li>更新X~i~为采样值</li>
    </ul>
  </li>
  <li>得到x[m]
    <h4 id="gibbs-sampling-for-bns">Gibbs Sampling for BNs</h4>
    <p><strong>采样过程</strong>
<img src="https://img-blog.csdnimg.cn/20181222192951919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIwNjA3MTg5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
    <h4 id="gibbs-sampling-for-mns">Gibbs Sampling for MNs</h4>
    <p><strong>采样过程</strong>
<img src="https://img-blog.csdnimg.cn/20181222193053624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIwNjA3MTg5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
    <h3 id="metropolis-hastings-algorithm">Metropolis-Hastings Algorithm</h3>
    <p>Gibbs Sampling 给定一个当前状态 S，转移到下一个状态的转移概率是确定的，整个状态转移概率矩阵其实是确定的。
现在 MH algorithm 追求的是可以从任意一个转移分布中采到下一个样本。和 Importance sampling 有点像，可以从任意的函数 Q 中去拟合 P。
所以设计了一个新的因子：acceptance probability 接受概率。是指是否接受一个状态转移 A(x→x’)
现在的转移变成：
状态 x 转移到状态 x’：T(x→x’) = T^Q^(x→x’)A(x→x’)
状态 x 停留在原状态：T(x→x) = T^Q^(x→x) + <script type="math/tex">\sum_{x'\neq x}</script>T^Q^(x→x’)(1-A(x→x’))
上式第一项是原来的转移就是停留在原状态，第二项是指本来要转到其他状态，但是被拒绝了，只能留在原状态。
因为 MC 是稳态的，所以互相转移的概率是相等的。
<script type="math/tex">\pi(x)T^Q(x→x')A(x→x')=\pi(x')T^Q(x'→x)A(x'→x)</script>
从上式推出：重要！！！在结构学习的时候会使用到这个结论<script type="math/tex">A(x'→x)=min[1, \frac{\pi(x')T^Q(x'→x)}{\pi(x)T^Q(x→x')}]</script>
在连续概率分布结论也是成立的。经常用到的是把转移概率分布设为多元高斯分布（可以看成是以当前状态为中心的随机游走）。那么此时的转移概率是对称的（随机游走过程），即T(x’→x)=T(x→x’)。
此时的 acceptance rate 只与稳态分布或者目标分布有关，即<script type="math/tex">A(x'→x)=min[1, \frac{\pi(x')}{\pi(x)}]=min[1, \frac{p(x')}{p(x)}]</script>
MH algorithm 有个缺陷就是不同状态之间的转移太低效了，可能会产生很多 rejection。</p>
  </li>
</ol>

<h3 id="hybrid-hamiltonian-monte-carlo-hmc">(Hybrid) Hamiltonian Monte Carlo (HMC)</h3>


            <!-- Comments -->
            

        </div>

        <div class="col-md-4">
            <h3>Post Directory</h3>
<div id="post-directory-module">
<section class="post-directory">
    <!-- Links that trigger the jumping -->
    <!-- Added by javascript below -->
    <dl></dl>
</section>
</div>

<script type="text/javascript">

    $(document).ready(function(){
        $( "article h2" ).each(function( index ) {
            $(".post-directory dl").append("<dt><a class=\"jumper\" hre=#" +
                    $(this).attr("id")
                    + ">"
                    + $(this).text()
                    + "</a></dt>");

            var children = $(this).nextUntil("h2", "h3")

            children.each(function( index ) {
                $(".post-directory dl").append("<dd><a class=\"jumper\" hre=#" +
                        $(this).attr("id")
                        + ">"
                        + "&nbsp;&nbsp;- " + $(this).text()
                        + "</a></dd>");
            });
        });

        var fixmeTop = $('#post-directory-module').offset().top - 100;       // get initial position of the element

        $(window).scroll(function() {                  // assign scroll event listener

            var currentScroll = $(window).scrollTop(); // get current position

            if (currentScroll >= fixmeTop) {           // apply position: fixed if you
                $('#post-directory-module').css({      // scroll to that element or below it
                    top: '100px',
                    position: 'fixed',
                    width: 'inherit'
                });
            } else {                                   // apply position: static
                $('#post-directory-module').css({      // if you scroll above it
                    position: 'inherit',
                    width: 'inherit'
                });
            }

        });

        $("a.jumper").on("click", function( e ) {

            e.preventDefault();

            $("body, html").animate({
                scrollTop: ($( $(this).attr('hre') ).offset().top - 100)
            }, 600);

        });
    });

</script>
        </div>
        

    </div>



        </div>

    <footer class="container">

    <div class="site-footer">

        <div class="copyright pull-left">
            <!-- 请不要更改这一行 方便其他人知道模板的来源 谢谢 -->
            <!-- Please keep this line to let others know where this theme comes from. Thank you :D -->
            Power by <a href="https://github.com/DONGChuan/Yummy-Jekyll">Yummy Jekyll</a>
        </div>

        <a href="https://github.com/strongman1995" target="_blank" aria-label="view source code">
            <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
        </a>

        <div align=center>
            Visitors:
            <a href='https://www.counter12.com'>
                <img src='https://www.counter12.com/img-Cwz4393YWx8CY2a8-15.gif' border='0' alt='free web counter'></a>
            <script type='text/javascript' src='https://www.counter12.com/ad.js?id=Cwz4393YWx8CY2a8'></script>
        </div>

        <div class="pull-right">
            <a href="javascript:window.scrollTo(0,0)">TOP</a>
        </div>

    </div>

    <!-- Third-Party JS -->
    <script type="text/javascript" src="/bower_components/geopattern/js/geopattern.min.js"></script>

    <!-- My JS -->
    <script type="text/javascript" src="/assets/js/script.js"></script>

    

    
    <!-- Google Analytics -->
    <!--<div style="display:none">-->
    <div>
        <script>
            (function (i, s, o, g, r, a, m) {
                i['GoogleAnalyticsObject'] = r;
                i[r] = i[r] || function () {
                    (i[r].q = i[r].q || []).push(arguments)
                }, i[r].l = 1 * new Date();
                a = s.createElement(o),
                    m = s.getElementsByTagName(o)[0];
                a.async = 1;
                a.src = g;
                m.parentNode.insertBefore(a, m)
            })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

            ga('create', 'UA-139849944-1', 'auto');
            ga('send', 'pageview');

        </script>
    </div>
    

</footer>


    </body>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

</html>
